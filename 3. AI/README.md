# AI 강의 1차
![1013-1](./1013-1.png)
![1013-2](./1013-2.png)



# AI 강의 2차
![1014-1](./1014-1.png)
![1014-2](./1014-2.png)

1. 단순선형회귀
![1014-1](./1014-1.png)
![1014-2](./1014-2.png)
![1014-3](./1014-3.png)
![1014-4](./1014-4.png)
![1014-5](./1014-5.png)
![1014-6](./1014-6.png)

2. 로지스틱 
![1014-7](./1014-7.png)
![1014-8](./1014-8.png)
![1014-9](./1014-9.png)


3. Shallow network  (앝은 신경망)
![1014-10](./1014-10.png)
![1014-11](./1014-11.png)
![1014-12](./1014-12.png)
![1014-13](./1014-13.png)
![1014-14](./1014-14.png)
![1014-15](./1014-15.png)

- Shallow: 숨은층(히든 레이어)이 1개.
- Deep: 숨은층이 여러 개(보통 3개 이상을 deep이라 부름).


4. Deep network (다 신경망)

- 히든레이어 없이 직접연결? 
- 과적합(오버피팅) 위험↓, 가볍고 해석 가능하며 안정적입니다. 다만 표현력이 낮아 데이터가 복잡할수록 한계가 빨리 와요.
![1014-16](./1014-16.png)
![1014-17](./1014-17.png)

## (결과) 많아질수록, 유연한 곡선이 도출
5. 손실함수
![1014-18](./1014-18.png)
* 이후 드랍   



# AI 강의 3차 - 자연어처리 
![1015-1](./1015-1.png)
1.
## 원-핫 인코딩 
![1015-2](./1015-2.png)
 - 원-핫 인코딩에서 은행과 금융이 독립적인 벡터로 표현되나,
 - 워드 임베딩에서 두 단어의 벡터가 공간상 서로 가깝게 위치하여, 의미적 유사성을 보인다. 

2. 워드 임베딩
![1015-3](./1015-3.png)
![1015-4](./1015-4.png)
  - output과 예측 간의 오차, Loss를 최소화하기 위해 연산한다. 
![1015-5](./1015-5.png)


1. 순차적 데이터 
일반적 모델로 처리 불가 -> 시퀀스 모델 필요
-> 한계: 입력이 가변적이나 출력이 고정됨. 
![1015-6](./1015-6.png)
시퀀스 데이터를 받아 -> RNN LSTM .... 

# RNN VS LSTM(중요) 
![1015-7](./1015-7.png) 
![1015-8](./1015-8.png) 
# LSTM, 기울기 소실문제를 해결하기 위해 등장! 
![1015-9](./1015-9.png) 

RNN은 입력 정보를 거부할 수 없으나, LSTM은 입력 정보를 거부할 수 있다. 
왜냐하면, Forget gate가 무엇을 버리고 무엇을 유지할 지 결정하기 때문이다.  

![1015-10](./1015-10.png) 

# 언어 모델
다음 단어를 예측 
![1015-11](./1015-11.png) 
![1015-12](./1015-12.png) 

# Seq2Seq 
![1015-13](./1015-13.png)
* encoder -> decoder  -> 문장 생성
* 예시 ; 자연어 처리보다는 코드 등 다양하게 응용됨. 
  * 1. 긴문서 읽고 짧게 요약 보고
  * 2. 입력(질문) -> 출력(대답) ; ai챗봇
* 정답 단어를 디코더 강제입력(Teacher Forcing) -> 안정적 
* Greedy Inference ; 높은 확률이 단어 선택 
* Beam Search ; 매 단계마다 k개의 유망 후보를 선택. 

# Attention 
* 700페이지를 읽은 뒤 덮고 번역한다고 하면, "정교한 번역이 불가능" 
* -> 번역을 하나하나 해야할 것 
![1015-14](./1015-14.png)
* 장점: 
  * 1. 디코더가 필요부분에 집중 가능. 
  * 2. 멀리 떨어진 단어, hidden states 각각에 직접 연결 가능.
* 효과: 강력하다! 
  * 1. 해석 가능성: 장점1에서 어느 부분에 집중했는 지 파악할 수 있다. 따라서 사람이 모델의 참고 근거를 파악할 수 있어, 의사결정 과정을 해석할 수 있다. 
  * 2. 정렬 : 디코더가 필요한 단어에 자동 집중한다. 단어간 매핑 관계를 자연스럽게 학습한다. 
![1015-15](./1015-15.png)

# Self-Attention
![1015-16](./1015-16.png)
![1015-17](./1015-17.png)

# Transformer
![1015-18](./1015-18.png)
![1015-19](./1015-19.png)
